{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nielsrolf/ddsp/blob/master/ddsp/colab/experiments/05_timbre_interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "C3cafbbjdGOU"
   },
   "outputs": [],
   "source": [
    "#@title #Install and Import\n",
    "\n",
    "!pip install git+git://github.com/nielsrolf/ddsp &> /dev/null\n",
    "\n",
    "\n",
    "\n",
    "#@markdown Install ddsp, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
    "%tensorflow_version 2.x\n",
    "\n",
    "# Ignore a bunch of deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import crepe\n",
    "import ddsp\n",
    "import ddsp.training\n",
    "from ddsp.colab import colab_utils\n",
    "from ddsp.colab.colab_utils import (\n",
    "    auto_tune, detect_notes, fit_quantile_transform, \n",
    "    get_tuning_factor, download, play, record, \n",
    "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
    "import gin\n",
    "from google.colab import files\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Helper Functions\n",
    "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVb3ztPHdqGE"
   },
   "outputs": [],
   "source": [
    "#@title #Mount drive or sync s3\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "#@markdown If you sync an s3 bucket, you will be asked for access key id and secret access key\n",
    "sync_s3 = True #@param{type:\"boolean\"}\n",
    "mount_gdrive = True #@param{type:\"boolean\"}\n",
    "\n",
    "\n",
    "if sync_s3:\n",
    "    results_dir = \"s3\"\n",
    "    s3_bucket = \"s3://niels-warncke-experiments\"\n",
    "    !pip install awscli &> /dev/null\n",
    "    os.makedirs(\"/root/.aws\", exist_ok=True)\n",
    "    with open(\"/root/.aws/credentials\", \"w\") as private_key:\n",
    "        print(\"aws_access_key_id\")\n",
    "        private_key.write(f\"[default]\\naws_access_key_id = {getpass.getpass()}\\n\")\n",
    "        print(\"aws_secret_access_key\")\n",
    "        private_key.write(f\"aws_secret_access_key = {getpass.getpass()}\\n\")\n",
    "    !aws s3 sync {s3_bucket} {results_dir} > /dev/null && rm -r /root/.aws\n",
    "    os.environ[\"URMP_MONO\"] = \"s3/urmp-mono/*\"\n",
    "    os.environ[\"RESULTS_DIR\"] = \"s3/models\"\n",
    "if mount_gdrive:\n",
    "    from google.colab import drive\n",
    "    os.environ['URMP_MONO'] = \"drive/MyDrive/ddsp/urmp-mono/*\"\n",
    "    os.environ['results_dir'] = 'drive/MyDrive/ddsp/models'\n",
    "    drive.mount('/content/drive')\n",
    "    #@markdown (ex. `/content/drive/MyDrive/...`) \n",
    "    DRIVE_DIR = 'drive/MyDrive/ddsp' #@param {type: \"string\"}\n",
    "    assert os.path.exists(DRIVE_DIR)\n",
    "    print('Drive Folder Exists:', DRIVE_DIR)\n",
    "    results_dir = DRIVE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnws4gnE4SBQ"
   },
   "outputs": [],
   "source": [
    "!cd s3 && unzip mono-instruments.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-V5OxYuw6qYO"
   },
   "outputs": [],
   "source": [
    "print(\"Select a model:\")\n",
    "!ls {results_dir}/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9rWk8Hn6zUA"
   },
   "outputs": [],
   "source": [
    "model = \"ddsp_constant_z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kKmeKy9do3Si"
   },
   "outputs": [],
   "source": [
    "#@title Record or Upload Source Audio\n",
    "#@markdown **Source Audio - we will use this melody and loudness**\n",
    "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
    "#@markdown * Audio should be monophonic (single instrument / voice)\n",
    "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
    "\n",
    "source = \"File System\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\", \"Youtube\", \"File System\"]\n",
    "\n",
    "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
    "\n",
    "youtube_url = \"https://www.youtube.com/watch?v=XvVmZmMLojc\" #@param {type:\"string\"}\n",
    "\n",
    "filename = \"/content/s3/samples/vozes.mp3\" #@param {type:\"string\"}\n",
    "\n",
    "model_dir = os.path.join(results_dir, \"models\", model)\n",
    "\n",
    "if source == \"Record\":\n",
    "    audio = record(seconds=record_seconds)\n",
    "elif source == \"Upload\":\n",
    "    # Load audio sample here (.mp3 or .wav3 file)\n",
    "    # Just use the first file.\n",
    "    filenames, audios = upload()\n",
    "    audio = audios[0]\n",
    "elif source == \"Youtube\":\n",
    "    !pip install youtube-dl &> /dev/null\n",
    "    from uuid import uuid4\n",
    "    import time\n",
    "    from glob import glob\n",
    "    filename = f\"{uuid4().hex[:5]}.mp3\"\n",
    "    files = set(glob('*'))\n",
    "    !youtube-dl --extract-audio {youtube_url} --audio-format mp3\n",
    "    time.sleep(10)\n",
    "    filename = (set(glob('*')) - files).pop()\n",
    "    source = \"File System\"\n",
    "if source == \"File System\":\n",
    "    !pip install pydub &> /dev/null\n",
    "    from pydub import AudioSegment\n",
    "    if filename.endswith(\".mp3\"):\n",
    "        song = AudioSegment.from_mp3(filename)\n",
    "    elif filename.endswith(\".wav\"):\n",
    "        song = AudioSegment.from_wav(filename)\n",
    "    audio = np.array(song.set_frame_rate(sample_rate).get_array_of_samples()).reshape(song.channels, -1, order='F')[0]\n",
    "    audio = audio / np.max(np.absolute(audio))\n",
    "audio_src = audio[np.newaxis, :]\n",
    "\n",
    "print(\"Original src audio\")\n",
    "play(audio_src, sample_rate=sample_rate)\n",
    "\n",
    "\n",
    "\n",
    "def load_model_for_audio_size(model_dir, audio):\n",
    "    start_time = time.time()\n",
    "    audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
    "    audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
    "    print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
    "\n",
    "    gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
    "\n",
    "    # Parse gin config,\n",
    "    with gin.unlock_config():\n",
    "        gin.parse_config_file(gin_file, skip_unknown=True)\n",
    "\n",
    "    # Use latest checkpoint in the folder, 'ckpt-[iter]`.\n",
    "    ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
    "    step_of = lambda f: int(f.split('.')[0].split('-')[1])\n",
    "    latest = max([step_of(f) for f in ckpt_files])\n",
    "    ckpt_name = [i for i in ckpt_files if step_of(i) == latest][0].split('.')[0]\n",
    "    ckpt = os.path.join(model_dir, ckpt_name)\n",
    "\n",
    "    # Ensure dimensions and sampling rates are equal\n",
    "    time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
    "    n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
    "    hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "    time_steps = int(audio.shape[1] / hop_size)\n",
    "    n_samples = time_steps * hop_size\n",
    "\n",
    "\n",
    "    # -----------  Load Model for decoding ----------------\n",
    "    gin_params = [\n",
    "        'Harmonic.n_samples = {}'.format(n_samples),\n",
    "        'FilteredNoise.n_samples = {}'.format(n_samples),\n",
    "        'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
    "        'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
    "    ]\n",
    "\n",
    "    with gin.unlock_config():\n",
    "        gin.parse_config(gin_params)\n",
    "\n",
    "    # Trim all input vectors to correct lengths \n",
    "    for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
    "        audio_features[key] = audio_features[key][:time_steps]\n",
    "    audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
    "\n",
    "\n",
    "    # Set up the model just to predict audio given new conditioning\n",
    "    start_time = time.time()\n",
    "    model = ddsp.training.models.Autoencoder()\n",
    "    model.restore(ckpt)\n",
    "\n",
    "    # Build model by running a batch through it.\n",
    "    out = model(audio_features, training=False)\n",
    "    print('Restoring model took %.1f seconds' % (time.time() - start_time))\n",
    "    return model, audio_features, out\n",
    "\n",
    "model_src, src_features, src_out = load_model_for_audio_size(model_dir, audio_src)\n",
    "print(\"Reconstruction\")\n",
    "play(src_out['audio_synth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dxC55ALWAr67"
   },
   "outputs": [],
   "source": [
    "#@markdown **Target Audio - we will use this to extract the timbre**\n",
    "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
    "#@markdown * Audio should be monophonic (single instrument / voice)\n",
    "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
    "\n",
    "source = \"File System\"  #@param [\"Upload (.mp3 or .wav)\", \"Youtube\", \"File System\"]\n",
    "\n",
    "youtube_url = \"https://www.youtube.com/watch?v=XvVmZmMLojc\" #@param {type:\"string\"}\n",
    "\n",
    "filename = \"/content/s3/mono-instruments/AuSep_4_cl_37_Rondeau.wav\" #@param {type:\"string\"}\n",
    "\n",
    "if source == \"Record\":\n",
    "    audio = record(seconds=record_seconds)\n",
    "elif source == \"Upload\":\n",
    "    # Load audio sample here (.mp3 or .wav3 file)\n",
    "    # Just use the first file.\n",
    "    filenames, audios = upload()\n",
    "    audio = audios[0]\n",
    "elif source == \"Youtube\":\n",
    "    !pip install youtube-dl &> /dev/null\n",
    "    from uuid import uuid4\n",
    "    import time\n",
    "    from glob import glob\n",
    "    filename = f\"{uuid4().hex[:5]}.mp3\"\n",
    "    files = set(glob('*'))\n",
    "    !youtube-dl --extract-audio {youtube_url} --audio-format mp3\n",
    "    time.sleep(10)\n",
    "    filename = (set(glob('*')) - files).pop()\n",
    "    source = \"File System\"\n",
    "if source == \"File System\":\n",
    "    !pip install pydub &> /dev/null\n",
    "    from pydub import AudioSegment\n",
    "    if filename.endswith(\".mp3\"):\n",
    "        song = AudioSegment.from_mp3(filename)\n",
    "    elif filename.endswith(\".wav\"):\n",
    "        song = AudioSegment.from_wav(filename)\n",
    "    audio = np.array(song.set_frame_rate(sample_rate).get_array_of_samples()).reshape(song.channels, -1, order='F')[0]\n",
    "    audio = audio / np.max(np.absolute(audio))\n",
    "audio_target = audio[np.newaxis, :]\n",
    "\n",
    "print(\"Original audio for target timbre:\")\n",
    "play(audio_target, sample_rate=sample_rate)\n",
    "\n",
    "print(\"Reconstruction\")\n",
    "model_target, target_features, target_out = load_model_for_audio_size(model_dir, audio_target)\n",
    "play(target_out['audio_synth'])\n",
    "\n",
    "\n",
    "from ddsp.colab.colab_utils import fit_quantile_transform, detect_notes\n",
    "\n",
    "na = None\n",
    "\n",
    "\n",
    "def get_dataset_statistics(audio_features):\n",
    "    f0 = audio_features['f0_hz'][na]\n",
    "    loudness = audio_features['loudness_db'][na]\n",
    "    f0_conf = audio_features['f0_confidence'][na]\n",
    "    trim_end = 20\n",
    "    f0_trimmed = f0[:, :-trim_end]\n",
    "    l_trimmed = loudness[:, :-trim_end]\n",
    "    f0_conf_trimmed = f0_conf[:, :-trim_end]\n",
    "    mask_on, _ = detect_notes(l_trimmed, f0_conf_trimmed)\n",
    "    quantile_transform = fit_quantile_transform(l_trimmed, mask_on)\n",
    "\n",
    "    # Average values.\n",
    "    mean_pitch = np.mean(ddsp.core.hz_to_midi(f0_trimmed[mask_on]))\n",
    "    mean_loudness = np.mean(l_trimmed)\n",
    "    mean_max_loudness = np.mean(np.max(l_trimmed, axis=0))\n",
    "\n",
    "    # Object to pickle all the statistics together.\n",
    "    ds = {'mean_pitch': mean_pitch,\n",
    "        'mean_loudness': mean_loudness,\n",
    "        'mean_max_loudness': mean_max_loudness,\n",
    "        'quantile_transform': quantile_transform}\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1RybabIwGsP-"
   },
   "outputs": [],
   "source": [
    "#@title Auto Adjust Settings\n",
    "#@markdown You can leave this at 1.0 for most cases\n",
    "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
    "\n",
    "\n",
    "#@markdown ## Automatic\n",
    "\n",
    "ADJUST = True #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown Quiet parts without notes detected (dB)\n",
    "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
    "\n",
    "#@markdown Force pitch to nearest note (amount)\n",
    "autotune = 0.1 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
    "\n",
    "#@markdown ## Manual\n",
    "\n",
    "\n",
    "#@markdown Shift the pitch (octaves)\n",
    "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
    "\n",
    "#@markdown Adjsut the overall loudness (dB)\n",
    "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
    "\n",
    "\n",
    "\n",
    "mixing_factor = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "\n",
    "#@title Magic\n",
    "# Auto Adjust\n",
    "\n",
    "## Helper functions.\n",
    "def shift_ld(audio_features, ld_shift=0.0):\n",
    "    \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
    "    audio_features['loudness_db'] += ld_shift\n",
    "    return audio_features\n",
    "\n",
    "\n",
    "def shift_f0(audio_features, pitch_shift=0.0):\n",
    "    \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
    "    audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
    "    audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
    "                                    0.0, \n",
    "                                    librosa.midi_to_hz(110.0))\n",
    "    return audio_features\n",
    "\n",
    "\n",
    "def auto_adjust(audio_features, src_statistics, target_statistics, mixing_factor):\n",
    "    # Detect sections that are \"on\".\n",
    "    audio_features_mod = {k: tf.identity(v) for k, v in audio_features.items()}\n",
    "    mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
    "                                        audio_features['f0_confidence'],\n",
    "                                        threshold)\n",
    "\n",
    "    if np.any(mask_on):\n",
    "        # Shift the pitch register.\n",
    "        target_mean_pitch = src_statistics['mean_pitch'] * (1 - mixing_factor) + \\\n",
    "                            target_statistics['mean_pitch'] * mixing_factor\n",
    "        pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
    "        mean_pitch = np.mean(pitch[mask_on])\n",
    "        p_diff = target_mean_pitch - mean_pitch\n",
    "        p_diff_octave = p_diff / 12.0\n",
    "        round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
    "        p_diff_octave = round_fn(p_diff_octave)\n",
    "        audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
    "\n",
    "        # Quantile shift the note_on parts.\n",
    "        _, loudness_norm_src = colab_utils.fit_quantile_transform(\n",
    "            audio_features['loudness_db'],\n",
    "            mask_on,\n",
    "            inv_quantile=src_statistics['quantile_transform'])\n",
    "        \n",
    "        _, loudness_norm_target = colab_utils.fit_quantile_transform(\n",
    "            audio_features['loudness_db'],\n",
    "            mask_on,\n",
    "            inv_quantile=target_statistics['quantile_transform'])\n",
    "        loudness_norm = loudness_norm_src * (1 - mixing_factor) + \\\n",
    "                        loudness_norm_target * mixing_factor\n",
    "\n",
    "        # Turn down the note_off parts.\n",
    "        mask_off = np.logical_not(mask_on)\n",
    "        loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
    "        loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
    "        \n",
    "        audio_features_mod['loudness_db'] = loudness_norm \n",
    "\n",
    "        # Auto-tune.\n",
    "        if autotune:\n",
    "            f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
    "            tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
    "            f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
    "            audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
    "        return audio_features_mod\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_features(mixing_factor=mixing_factor, mixing_features=['z']):\n",
    "    \"\"\"works on the global variables for the models and features\"\"\"\n",
    "\n",
    "    interpolation_latents = {k: v.copy() for k, v in src_features.items()}\n",
    "    # Manual Shifts.\n",
    "    interpolation_latents = shift_ld(interpolation_latents, loudness_shift)\n",
    "    interpolation_latents = shift_f0(interpolation_latents, pitch_shift)\n",
    "\n",
    "    mask_on, note_on_value = detect_notes(target_features['loudness_db'],\n",
    "                                            target_features['f0_confidence'],\n",
    "                                            threshold)\n",
    "    # Feature interpolations\n",
    "    for feature in mixing_features:\n",
    "        # interpolation_latents[feature] = src_out[feature] * (1 - mixing_factor) + \\\n",
    "        #     tf.reduce_mean(target_out[feature], axis=1, keepdims=True) * mixing_factor\n",
    "        # weighted mean\n",
    "        w = target_out['ld_scaled'] * tf.reshape(target_out['f0_confidence'], [1, -1, 1])\n",
    "        w = tf.cast(tf.reshape(mask_on, [1, -1, 1]), tf.float32)\n",
    "\n",
    "\n",
    "        t = tf.reduce_sum(target_out[feature]*w, axis=1, keepdims=True)\\\n",
    "            /tf.reduce_sum(w)\n",
    "        interpolation_latents[feature] = src_out[feature] * (1 - mixing_factor) + \\\n",
    "            t * mixing_factor\n",
    "        \n",
    "    # Auto adjust\n",
    "    target_statistics = get_dataset_statistics(target_features)\n",
    "    src_statistics = get_dataset_statistics(src_features)\n",
    "    interpolation_latents = auto_adjust(interpolation_latents, src_statistics, target_statistics, mixing_factor)\n",
    "\n",
    "    if model_src.preprocessor is not None:\n",
    "        interpolation_latents.update(model_src.preprocessor(interpolation_latents, training=False))\n",
    "\n",
    "    interpolation_latents.update(model_src.decoder(interpolation_latents))\n",
    "    pg_out = model_src.processor_group(interpolation_latents, return_outputs_dict=True)\n",
    "    interpolation_audio = pg_out['signal']\n",
    "\n",
    "    play(interpolation_audio)\n",
    "\n",
    "interpolate_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dbhxt1SCLMhQ"
   },
   "outputs": [],
   "source": [
    "for mixing_factor in [0, 0.25, 0.5, 0.75, 1]:\n",
    "    interpolate_features(mixing_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8fs844q-jkv"
   },
   "source": [
    "Let's look at how much time varying information is encoded in z (only makes sense if the `MfccTimeDistributedEncoder` was used). Ideally the entries of z are constant over time if a single instrument is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqF21XJXG64P"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "def smooth(y, window=None):\n",
    "    window = window or min(max(len(y)//30, 2), 20)\n",
    "    return np.convolve(np.ones(window)/window, y, mode='same')\n",
    "\n",
    "for i in range(16):\n",
    "    plt.plot(smooth(target_out['z'][0,:,i], 1), label=i, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG5vNHc0cRAa"
   },
   "source": [
    "Notes:\n",
    "- If we do timbre transfer from one file onto itself, it sounds quite bad although the model can reconstruct the sample really good. Does the sound depend too much on z? Or is it the auto_adjust? -> Auto adjust is not the reason, so I should train a model that can only use a constant z over the time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwbcsopRaWhy"
   },
   "outputs": [],
   "source": [
    "pg_out['controls']['harmonic']['signal'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWjRKa9zaSVd"
   },
   "outputs": [],
   "source": [
    "# Only harmonics\n",
    "play(pg_out['controls']['harmonic']['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJgCngRabQ2l"
   },
   "outputs": [],
   "source": [
    "play(target_out['harmonic']['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x00U9iCHtPGs"
   },
   "outputs": [],
   "source": [
    "#@title # Uploading to s3\n",
    "\n",
    "os.makedirs(\"/root/.aws\", exist_ok=True)\n",
    "with open(\"/root/.aws/credentials\", \"w\") as private_key:\n",
    "    print(\"aws_access_key_id\")\n",
    "    private_key.write(f\"[default]\\naws_access_key_id = {getpass.getpass()}\\n\")\n",
    "    print(\"aws_secret_access_key\")\n",
    "    private_key.write(f\"aws_secret_access_key = {getpass.getpass()}\\n\")\n",
    "!aws s3 sync {results_dir} {s3_bucket} && rm -r /root/.aws"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN2PG45K1UK3ihQjOM/p7Pd",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
