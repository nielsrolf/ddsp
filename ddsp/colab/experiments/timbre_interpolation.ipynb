{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbh6RTvU/wC07zTn/l3Pd3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nielsrolf/ddsp/blob/master/ddsp/colab/experiments/timbre_interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3cafbbjdGOU",
        "outputId": "9091138d-e4e7-4053-cf6e-75b6ded19046"
      },
      "source": [
        "#@title #Install and Import\n",
        "\n",
        "!pip install git+git://github.com/nielsrolf/ddsp &> /dev/null\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Install ddsp, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab import colab_utils\n",
        "from ddsp.colab.colab_utils import (\n",
        "    auto_tune, detect_notes, fit_quantile_transform, \n",
        "    get_tuning_factor, download, play, record, \n",
        "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "import gin\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
        "\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVb3ztPHdqGE",
        "outputId": "19cf4acf-d203-4259-e85e-f17e2d49798e"
      },
      "source": [
        "#@title #Mount drive or sync s3\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "sync_s3 = True\n",
        "\n",
        "\n",
        "if sync_s3:\n",
        "    results_dir = \"s3\"\n",
        "    s3_bucket = \"s3://niels-warncke-experiments\"\n",
        "    !pip install awscli &> /dev/null\n",
        "    os.makedirs(\"/root/.aws\", exist_ok=True)\n",
        "    with open(\"/root/.aws/credentials\", \"w\") as private_key:\n",
        "        print(\"aws_access_key_id\")\n",
        "        private_key.write(f\"[default]\\naws_access_key_id = {getpass.getpass()}\\n\")\n",
        "        print(\"aws_secret_access_key\")\n",
        "        private_key.write(f\"aws_secret_access_key = {getpass.getpass()}\\n\")\n",
        "    !aws s3 sync {s3_bucket} {results_dir} &> /dev/null && rm -r /root/.aws\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    #@markdown (ex. `/content/drive/MyDrive/...`) Leave blank to skip loading from Drive.\n",
        "    DRIVE_DIR = 'drive/MyDrive/ddsp' #@param {type: \"string\"}\n",
        "    assert os.path.exists(DRIVE_DIR)\n",
        "    print('Drive Folder Exists:', DRIVE_DIR)\n",
        "    results_dir = DRIVE_DIR"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aws_access_key_id\n",
            "··········\n",
            "aws_secret_access_key\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKmeKy9do3Si",
        "cellView": "form"
      },
      "source": [
        "#@title Record or Upload Source Audio\n",
        "#@markdown **Source Audio - we will use this melody and loudness**\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "\n",
        "source = \"File System\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\", \"Youtube\", \"File System\"]\n",
        "\n",
        "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "youtube_url = \"https://www.youtube.com/watch?v=XvVmZmMLojc\" #@param {type:\"string\"}\n",
        "\n",
        "filename = \"s3/samples/vozes.mp3\" #@param {type:\"string\"}\n",
        "\n",
        "if source == \"Record\":\n",
        "    audio = record(seconds=record_seconds)\n",
        "elif source == \"Upload\":\n",
        "    # Load audio sample here (.mp3 or .wav3 file)\n",
        "    # Just use the first file.\n",
        "    filenames, audios = upload()\n",
        "    audio = audios[0]\n",
        "elif source == \"Youtube\":\n",
        "    !pip install youtube-dl &> /dev/null\n",
        "    from uuid import uuid4\n",
        "    import time\n",
        "    from glob import glob\n",
        "    filename = f\"{uuid4().hex[:5]}.mp3\"\n",
        "    files = set(glob('*'))\n",
        "    !youtube-dl --extract-audio {youtube_url} --audio-format mp3\n",
        "    time.sleep(10)\n",
        "    filename = (set(glob('*')) - files).pop()\n",
        "    source = \"File System\"\n",
        "if source == \"File System\":\n",
        "    !pip install pydub &> /dev/null\n",
        "    from pydub import AudioSegment\n",
        "    if filename.endswith(\".mp3\"):\n",
        "        song = AudioSegment.from_mp3(filename)\n",
        "    elif filename.endswith(\".wav\"):\n",
        "        song = AudioSegment.from_wav(filename)\n",
        "    audio = np.array(song.set_frame_rate(sample_rate).get_array_of_samples()).reshape(song.channels, -1, order='F')[0]\n",
        "    audio = audio / np.max(np.absolute(audio))\n",
        "audio_src = audio[np.newaxis, :]\n",
        "\n",
        "play(audio_src, sample_rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VvKFkn6BPjk"
      },
      "source": [
        "!cd s3 && unzip mono-instruments.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dxC55ALWAr67"
      },
      "source": [
        "#@markdown **Target Audio - we will use this to extract the timbre**\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "\n",
        "source = \"File System\"  #@param [\"Upload (.mp3 or .wav)\", \"Youtube\", \"File System\"]\n",
        "\n",
        "youtube_url = \"https://www.youtube.com/watch?v=XvVmZmMLojc\" #@param {type:\"string\"}\n",
        "\n",
        "filename = \"s3/mono-instruments/AuSep_1_cl_19_Pavane.wav\" #@param {type:\"string\"}\n",
        "\n",
        "if source == \"Record\":\n",
        "    audio = record(seconds=record_seconds)\n",
        "elif source == \"Upload\":\n",
        "    # Load audio sample here (.mp3 or .wav3 file)\n",
        "    # Just use the first file.\n",
        "    filenames, audios = upload()\n",
        "    audio = audios[0]\n",
        "elif source == \"Youtube\":\n",
        "    !pip install youtube-dl &> /dev/null\n",
        "    from uuid import uuid4\n",
        "    import time\n",
        "    from glob import glob\n",
        "    filename = f\"{uuid4().hex[:5]}.mp3\"\n",
        "    files = set(glob('*'))\n",
        "    !youtube-dl --extract-audio {youtube_url} --audio-format mp3\n",
        "    time.sleep(10)\n",
        "    filename = (set(glob('*')) - files).pop()\n",
        "    source = \"File System\"\n",
        "if source == \"File System\":\n",
        "    !pip install pydub &> /dev/null\n",
        "    from pydub import AudioSegment\n",
        "    if filename.endswith(\".mp3\"):\n",
        "        song = AudioSegment.from_mp3(filename)\n",
        "    elif filename.endswith(\".wav\"):\n",
        "        song = AudioSegment.from_wav(filename)\n",
        "    audio = np.array(song.set_frame_rate(sample_rate).get_array_of_samples()).reshape(song.channels, -1, order='F')[0]\n",
        "    audio = audio / np.max(np.absolute(audio))\n",
        "audio_target = audio[np.newaxis, :]\n",
        "\n",
        "play(audio_target, sample_rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpC5GYQBwK6l",
        "cellView": "form"
      },
      "source": [
        "#@title Building the src model\n",
        "model_dir = os.path.join(results_dir, \"models\", model)\n",
        "\n",
        "def load_model_for_audio_size(model_dir, audio):\n",
        "    start_time = time.time()\n",
        "    audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
        "    audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "    print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "    gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "    # Parse gin config,\n",
        "    with gin.unlock_config():\n",
        "        gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "    # Use latest checkpoint in the folder, 'ckpt-[iter]`.\n",
        "    ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "    step_of = lambda f: int(f.split('.')[0].split('-')[1])\n",
        "    latest = max([step_of(f) for f in ckpt_files])\n",
        "    ckpt_name = [i for i in ckpt_files if step_of(i) == latest][0].split('.')[0]\n",
        "    ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "    # Ensure dimensions and sampling rates are equal\n",
        "    time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
        "    n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
        "    hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "    time_steps = int(audio.shape[1] / hop_size)\n",
        "    n_samples = time_steps * hop_size\n",
        "\n",
        "\n",
        "    # -----------  Load Model for decoding ----------------\n",
        "    gin_params = [\n",
        "        'Harmonic.n_samples = {}'.format(n_samples),\n",
        "        'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "        'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
        "        'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
        "    ]\n",
        "\n",
        "    with gin.unlock_config():\n",
        "        gin.parse_config(gin_params)\n",
        "\n",
        "    # Trim all input vectors to correct lengths \n",
        "    for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "        audio_features[key] = audio_features[key][:time_steps]\n",
        "    audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "\n",
        "    # Set up the model just to predict audio given new conditioning\n",
        "    start_time = time.time()\n",
        "    model = ddsp.training.models.Autoencoder()\n",
        "    model.restore(ckpt)\n",
        "\n",
        "    # Build model by running a batch through it.\n",
        "    out = model(audio_features, training=False)\n",
        "    print('Restoring model took %.1f seconds' % (time.time() - start_time))\n",
        "    return model, audio_features, out\n",
        "\n",
        "model_src, src_features, src_out = load_model_for_audio_size(model_dir, audio_src)\n",
        "play(src_out['audio_synth'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Xhayg0kqAFzP"
      },
      "source": [
        "#@title Target Timbre Audio\n",
        "\n",
        "model_target, target_features, target_out = load_model_for_audio_size(model_dir, audio_target)\n",
        "play(target_out['audio_synth'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MPBPBrTo2nY",
        "cellView": "form"
      },
      "source": [
        "#@title Get dataset statistics for auto adjustment\n",
        "from ddsp.colab.colab_utils import fit_quantile_transform, detect_notes\n",
        "\n",
        "na = None\n",
        "\n",
        "\n",
        "def get_dataset_statistics(audio_features):\n",
        "    f0 = audio_features['f0_hz'][na]\n",
        "    loudness = audio_features['loudness_db'][na]\n",
        "    f0_conf = audio_features['f0_confidence'][na]\n",
        "    trim_end = 20\n",
        "    f0_trimmed = f0[:, :-trim_end]\n",
        "    l_trimmed = loudness[:, :-trim_end]\n",
        "    f0_conf_trimmed = f0_conf[:, :-trim_end]\n",
        "    mask_on, _ = detect_notes(l_trimmed, f0_conf_trimmed)\n",
        "    quantile_transform = fit_quantile_transform(l_trimmed, mask_on)\n",
        "\n",
        "    # Average values.\n",
        "    mean_pitch = np.mean(ddsp.core.hz_to_midi(f0_trimmed[mask_on]))\n",
        "    mean_loudness = np.mean(l_trimmed)\n",
        "    mean_max_loudness = np.mean(np.max(l_trimmed, axis=0))\n",
        "\n",
        "    # Object to pickle all the statistics together.\n",
        "    ds = {'mean_pitch': mean_pitch,\n",
        "        'mean_loudness': mean_loudness,\n",
        "        'mean_max_loudness': mean_max_loudness,\n",
        "        'quantile_transform': quantile_transform}\n",
        "    return ds\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1RybabIwGsP-"
      },
      "source": [
        "#@title Auto Adjust Settings\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "\n",
        "#@markdown ## Automatic\n",
        "\n",
        "ADJUST = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0.4 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual\n",
        "\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "#@markdown Adjsut the overall loudness (dB)\n",
        "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "\n",
        "\n",
        "mixing_factor = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Ao52LHReI0Lx"
      },
      "source": [
        "#@title Magic\n",
        "# Auto Adjust\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "    \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "    audio_features['loudness_db'] += ld_shift\n",
        "    return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shift=0.0):\n",
        "    \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "    audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
        "    audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "    return audio_features\n",
        "\n",
        "\n",
        "def auto_adjust(audio_features, src_statistics, target_statistics, mixing_factor):\n",
        "    # Detect sections that are \"on\".\n",
        "    audio_features_mod = {k: tf.identity(v) for k, v in audio_features.items()}\n",
        "    mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
        "                                        audio_features['f0_confidence'],\n",
        "                                        threshold)\n",
        "\n",
        "    if np.any(mask_on):\n",
        "        # Shift the pitch register.\n",
        "        target_mean_pitch = src_statistics['mean_pitch'] * (1 - mixing_factor) + \\\n",
        "                            target_statistics['mean_pitch'] * mixing_factor\n",
        "        pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
        "        mean_pitch = np.mean(pitch[mask_on])\n",
        "        p_diff = target_mean_pitch - mean_pitch\n",
        "        p_diff_octave = p_diff / 12.0\n",
        "        round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
        "        p_diff_octave = round_fn(p_diff_octave)\n",
        "        audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
        "\n",
        "        # Quantile shift the note_on parts.\n",
        "        _, loudness_norm_src = colab_utils.fit_quantile_transform(\n",
        "            audio_features['loudness_db'],\n",
        "            mask_on,\n",
        "            inv_quantile=src_statistics['quantile_transform'])\n",
        "        \n",
        "        _, loudness_norm_target = colab_utils.fit_quantile_transform(\n",
        "            audio_features['loudness_db'],\n",
        "            mask_on,\n",
        "            inv_quantile=target_statistics['quantile_transform'])\n",
        "        loudness_norm = loudness_norm_src * (1 - mixing_factor) + \\\n",
        "                        loudness_norm_target * mixing_factor\n",
        "\n",
        "        # Turn down the note_off parts.\n",
        "        mask_off = np.logical_not(mask_on)\n",
        "        loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
        "        loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
        "        \n",
        "        audio_features_mod['loudness_db'] = loudness_norm \n",
        "\n",
        "        # Auto-tune.\n",
        "        if autotune:\n",
        "            f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "            tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
        "            f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
        "            audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
        "        return audio_features_mod\n",
        "\n",
        "\n",
        "\n",
        "# def interpolate_features(src_features, target_out, mixing_factor=0.5, mixing_features=['z']):\n",
        "mixing_factor=1.0\n",
        "mixing_features=['z']\n",
        "\n",
        "interpolation_latents = {k: v.copy() for k, v in src_features.items()}\n",
        "# Manual Shifts.\n",
        "interpolation_latents = shift_ld(interpolation_latents, loudness_shift)\n",
        "interpolation_latents = shift_f0(interpolation_latents, pitch_shift)\n",
        "# Feature interpolations\n",
        "for feature in mixing_features:\n",
        "    interpolation_latents[feature] = src_out[feature] * (1 - mixing_factor) + \\\n",
        "        tf.reduce_mean(target_out[feature], axis=1, keepdims=True) * mixing_factor\n",
        "# Auto adjust\n",
        "target_statistics = get_dataset_statistics(target_features)\n",
        "src_statistics = get_dataset_statistics(src_features)\n",
        "\n",
        "interpolation_latents = auto_adjust(interpolation_latents, src_statistics, target_statistics, mixing_factor)\n",
        "if model_src.preprocessor is not None:\n",
        "    interpolation_latents.update(model_src.preprocessor(interpolation_latents, training=False))\n",
        "\n",
        "interpolation_latents.update(model_src.decoder(interpolation_latents))\n",
        "pg_out = model_src.processor_group(interpolation_latents, return_outputs_dict=True)\n",
        "interpolation_audio = pg_out['signal']\n",
        "\n",
        "play(interpolation_audio)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x00U9iCHtPGs",
        "cellView": "form",
        "outputId": "0587f987-d255-4787-de41-37b5c1e10e4f"
      },
      "source": [
        "#@title # Uploading to s3\n",
        "\n",
        "os.makedirs(\"/root/.aws\", exist_ok=True)\n",
        "with open(\"/root/.aws/credentials\", \"w\") as private_key:\n",
        "    print(\"aws_access_key_id\")\n",
        "    private_key.write(f\"[default]\\naws_access_key_id = {getpass.getpass()}\\n\")\n",
        "    print(\"aws_secret_access_key\")\n",
        "    private_key.write(f\"aws_secret_access_key = {getpass.getpass()}\\n\")\n",
        "!aws s3 sync {results_dir} {s3_bucket} && rm -r /root/.aws"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aws_access_key_id\n",
            "··········\n",
            "aws_secret_access_key\n",
            "··········\n",
            "upload: s3/samples/vozes.mp3 to s3://niels-warncke-experiments/samples/vozes.mp3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}