
- Image for schema


# Questions
Todo:
- add scheduler
- add hyperparams for d_optimizer to trainer
- add hyperparams for d_optimizer to timbrepainting.gin
- pass conditioning info to d
- implement f0synthesizer for initial upsampler
- implement upsampler
- review discriminator conv layer

- add as features for the upsampler a bunch of sinus waves that have frequencies according to harmonics

# New Classes:

decoders.py:
    ParallelWaveGANUpsampler(Decoder)

discriminator.py

losses.py:
    AdversarialMSELoss(Loss)

trainer.py:
    __init__:
        - create discriminator optimizer
        - define a learning schedule for the different steps and number of generators to use in each step 

    scheduler:
        - create a tf.function with:
            - control flow to execute g or d step
            - control flow to set number of upsamplers
            - control flow only depends on tf.Tensor objects

    separate:
        gan.py from autoencoder.py
        GanTrainer from trainer

    


    step_fn: call to scheduler

        - Todo: tell the generator how many upsamplers to use
        - Todo here: get audio_real audio_gen

    step functions of different models:
        Autoencoder:
            - classical train step
        ParallelWaveGANUpsampler:
            - upsamplers are trained in stages
            - depending on the stage, the model decides how many upsamplers to use 
            - depending on that, the input is downsampled to the generators sample rate
            - on initialization of the next stage, weights from G[j-1] are copied to G[j]
        Discriminator:
            - classical train_step
        TimbrePainting:
            - sub models: ParallelWaveGANUpsampler, Discriminator
            - upsampler.train_step(batch)
            - split batch, reconstructions into audio_real, audio_gen
            - discriminator.train_step(audio_real, audio_gen)
            - instead of creating a new discriminator and copying the weights, we just continue to use the discriminator we already have


model.py:
    the losses dict has get an additional entry: 'discriminator_loss'


# Components:


MonoAutoencoder
    MonoEncoder:
        (MFCC) => (z, loudness)

        f0 Encoder:
            (MFCC) => (f0)
            CREPE
    
    FeatureDecoder:
        (f0, z) => ('amps', 'harmonic_distribution', 'noise_magnitudes')

    Synthesizer:Processor
        ('f0_hz', 'amps', 'harmonic_distribution', 'noise_magnitudes') => (audio)


GanDecoder(Decoder)
    discriminate(audio, conditions) => [0,1]
    discriminator_loss(real_audio, gen_audio) => loss, 


MonoTimbreUpsamblingDecoder(GanDecoder): (f0, loudness, z) => (audio)

    InitialSampler:Processor
        (f0, loudness) => (audio)

    ParallelWaveGANUpsampler:
        (f0, loudness, z, audio) => (audio)



PolyAutoEncoder
    PolyEncoder:
        (MFCC) => (z: (N_synth, z_dims, time), f0: (N_synth, time), loudness: (N_synth, time))

    PolyDecoder: (f0, loudness, z) => audio
        stacked applications of MonoDecoder, with shared weights



AdversarialLoss
    ParallelWaveGANDiscriminator:
        (audio) => [0,1]

    .train_step(audio_real, audio_gen)


DiscriminatorLoss:

