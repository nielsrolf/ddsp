# Questions
- why is scale_fn included in e.g. Additive synth? why wouldn't I just scale it in the network that produces the input
- where is this scale_fn actually used (by which applications)? Loudness is scaled by default, but isn't it computed by some fixed algorithm?

- why can almost all layers ignore the number of samples in the time axis?

Todo:
- timbre painting training stages will not be handled from the trainer loop. instead, trainer will be called for each stage, and we use some generic way to create a model from pretrained modules
- add scheduler?
    - maybe probabilistic, i.e. we randomly sample which steps to include, if there is no clean way to do it always the same 

- add hyperparams for d_optimizer to trainer
- add hyperparams for d_optimizer to timbrepainting.gin
- pass conditioning info to d
- implement upsampler
- review discriminator conv layer

- add as features for the upsampler a bunch of sinus waves that have frequencies according to harmonics


# New Classes:

synths.py:
    BasicUpsampler
        - Like additive synths without harmonics

decoders.py:
    TimbrePaintingDecoder(Decoder) <- BasicUpsampler, ParallelWaveGANUpsampler
        - Combines a BasicUpsampler and a stack of ParallelWaveGANUpsamplers
    ParallelWaveGANUpsampler
        - Architecture?

discriminator.py
    Discriminator
        - discriminator takes a dict with controls and the target audio
        - discriminator can decide which of these info to use by itself
        - with this setup, we can model any kind of conditioning information
    ParallelWaveGANDiscriminator(Discriminator)
        - does not use any conditioning features 


gan.py
    Implements the GAN train step, is otherwise like an autoencoder

losses.py:
    AdversarialMSELoss(Loss) <- Discriminator


trainer.py:
    __init__:
        - create discriminator optimizer
        - define a learning schedule for the different steps and number of generators to use in each step 

    scheduler:
        - create a tf.function with:
            - control flow to execute g or d step
            - control flow to set number of upsamplers
            - control flow only depends on tf.Tensor objects

    separate:
        gan.py from autoencoder.py
        GanTrainer from trainer

    


    step_fn: call to scheduler

        - Todo: tell the generator how many upsamplers to use
        - Todo here: get audio_real audio_gen

    step functions of different models:
        Autoencoder:
            - classical train step
        ParallelWaveGANUpsampler:
            - can take another upsampler to copy initial weights
            - downsamples target to output sample rate
        Discriminator:
            - classical train_step
        TimbrePainting:
            - sub models: ParallelWaveGANUpsampler, Discriminator
            - upsampler.train_step(batch)
            - split batch, reconstructions into audio_real, audio_gen
            - discriminator.train_step(audio_real, audio_gen)
            - instead of creating a new discriminator and copying the weights, we just continue to use the discriminator we already have


model.py:
    the losses dict has get an additional entry: 'discriminator_loss'


# Components:


MonoAutoencoder
    MonoEncoder:
        (MFCC) => (z, loudness)

        f0 Encoder:
            (MFCC) => (f0)
            CREPE
    
    FeatureDecoder:
        (f0, z) => ('amps', 'harmonic_distribution', 'noise_magnitudes')

    Synthesizer:Processor
        ('f0_hz', 'amps', 'harmonic_distribution', 'noise_magnitudes') => (audio)


GanDecoder(Decoder)
    discriminate(audio, conditions) => [0,1]
    discriminator_loss(real_audio, gen_audio) => loss, 


MonoTimbreUpsamblingDecoder(GanDecoder): (f0, loudness, z) => (audio)

    InitialSampler:Processor
        (f0, loudness) => (audio)

    ParallelWaveGANUpsampler:
        (f0, loudness, z, audio) => (audio)



PolyAutoEncoder
    PolyEncoder:
        (MFCC) => (z: (N_synth, z_dims, time), f0: (N_synth, time), loudness: (N_synth, time))

    PolyDecoder: (f0, loudness, z) => audio
        stacked applications of MonoDecoder, with shared weights



AdversarialLoss
    ParallelWaveGANDiscriminator:
        (audio) => [0,1]

    .train_step(audio_real, audio_gen)


DiscriminatorLoss:

